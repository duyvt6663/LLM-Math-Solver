{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = {\n",
    "#     \"question\": \"10% of 11.5m2 is:\",\n",
    "#     \"choices\": [\"A. 10,15dm2\", \"B. 1,5m2\", \"C. 15,5m2\", \"D. 1,15m2\"],\n",
    "# }\n",
    "\n",
    "example_sample = {\n",
    "    \"id\": \"xxxx\",\n",
    "\n",
    "\n",
    "    \n",
    "    # \"question\": \"A store sold 30% of its existing goods and earned 15,000,000 VND. If all goods were sold, how much money would the store earn?\",\n",
    "    # \"choices\": [\n",
    "    #     \"A. 4 500 000 VND\",\n",
    "    #     \"B. 45 000 000 VND\",\n",
    "    #     \"C. 50 000 000 VND\",\n",
    "    #     \"D. 450 000 000 VND\"\n",
    "    # ]\n",
    "    # \"question\": \"8 dm2 24 cm2 = ……… dm2. The appropriate number to fill in the blanks is:\",\n",
    "    #       \"choices\": [\n",
    "    #          \"A. 824\",\n",
    "    #          \"B. 82.4\",\n",
    "    #          \"C. 8.24\",\n",
    "    #          \"D. 0.824\"\n",
    "    #       ],\n",
    "    \n",
    "#     \"question\": \"2 ${\\\\times}$ 9 ? – 28 5 ${\\\\times}$ 3\",\n",
    "#     \"choices\": [\n",
    "#     \"A. 52\",\n",
    "#     \"B. 53\",\n",
    "#     \"C. 41\",\n",
    "#     \"D. 45\"\n",
    "#     ],\n",
    "    \n",
    "#     \"question\": \"The appropriate number to fill in the blanks 5kg 30g = …….. kg is:\",\n",
    "#           \"choices\": [\n",
    "#              \"A. 53\",\n",
    "#              \"B. 50.3\",\n",
    "#              \"C. 5.03\",\n",
    "#              \"D. 5.3\"\n",
    "#           ]\n",
    "\n",
    "    \"question\": \"A cyclist rode from A at 7 o'clock at a speed of 12km/h. At 8 o'clock a motorcyclist also from A chased the cyclist at a speed of 42km/h. Ask the cyclist What time did the machine catch up with the cyclist?\",\n",
    "          \"choices\": [\n",
    "             \"A. 24 minutes\",\n",
    "             \"B. 1 hour\",\n",
    "             \"C. 7 hours 24 minutes\",\n",
    "             \"D. 8 hours 24 minutes\"\n",
    "          ]\n",
    "    # \"question\": \"Một cửa hàng đã bán 30% số hàng hiện có và thu được 15 000 000 đồng. Hỏi nếu bán hết hàng thì cửa hàng thu được bao nhiêu tiền?\",\n",
    "    #     \"choices\": [\n",
    "    #         \"A. 4 500 000 đồng\",\n",
    "    #         \"B. 45 000 000 đồng\",\n",
    "    #         \"C. 50 000 000 đồng\",\n",
    "    #         \"D. 450 000 000 đồng\"\n",
    "    #     ]\n",
    "}\n",
    "\n",
    "answer(example_sample, call_mistral_math, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import json\n",
    "from llama_cpp import Llama\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from langdetect import detect\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "CACHE_DIR = \"./cache\"\n",
    "\n",
    "dict_map = {\n",
    "    \"òa\": \"oà\",\n",
    "    \"Òa\": \"Oà\",\n",
    "    \"ÒA\": \"OÀ\",\n",
    "    \"óa\": \"oá\",\n",
    "    \"Óa\": \"Oá\",\n",
    "    \"ÓA\": \"OÁ\",\n",
    "    \"ỏa\": \"oả\",\n",
    "    \"Ỏa\": \"Oả\",\n",
    "    \"ỎA\": \"OẢ\",\n",
    "    \"õa\": \"oã\",\n",
    "    \"Õa\": \"Oã\",\n",
    "    \"ÕA\": \"OÃ\",\n",
    "    \"ọa\": \"oạ\",\n",
    "    \"Ọa\": \"Oạ\",\n",
    "    \"ỌA\": \"OẠ\",\n",
    "    \"òe\": \"oè\",\n",
    "    \"Òe\": \"Oè\",\n",
    "    \"ÒE\": \"OÈ\",\n",
    "    \"óe\": \"oé\",\n",
    "    \"Óe\": \"Oé\",\n",
    "    \"ÓE\": \"OÉ\",\n",
    "    \"ỏe\": \"oẻ\",\n",
    "    \"Ỏe\": \"Oẻ\",\n",
    "    \"ỎE\": \"OẺ\",\n",
    "    \"õe\": \"oẽ\",\n",
    "    \"Õe\": \"Oẽ\",\n",
    "    \"ÕE\": \"OẼ\",\n",
    "    \"ọe\": \"oẹ\",\n",
    "    \"Ọe\": \"Oẹ\",\n",
    "    \"ỌE\": \"OẸ\",\n",
    "    \"ùy\": \"uỳ\",\n",
    "    \"Ùy\": \"Uỳ\",\n",
    "    \"ÙY\": \"UỲ\",\n",
    "    \"úy\": \"uý\",\n",
    "    \"Úy\": \"Uý\",\n",
    "    \"ÚY\": \"UÝ\",\n",
    "    \"ủy\": \"uỷ\",\n",
    "    \"Ủy\": \"Uỷ\",\n",
    "    \"ỦY\": \"UỶ\",\n",
    "    \"ũy\": \"uỹ\",\n",
    "    \"Ũy\": \"Uỹ\",\n",
    "    \"ŨY\": \"UỸ\",\n",
    "    \"ụy\": \"uỵ\",\n",
    "    \"Ụy\": \"Uỵ\",\n",
    "    \"ỤY\": \"UỴ\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_vi2en = AutoTokenizer.from_pretrained(\n",
    "    \"vinai/vinai-translate-vi2en\", src_lang=\"vi_VN\", cache_dir=\"./cache\"\n",
    ")\n",
    "model_vi2en = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"vinai/vinai-translate-vi2en\", cache_dir=\"./cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-math/MetaMath-Mistral-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    cache_dir=\"./cache\",\n",
    ")\n",
    "backbone_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=\"./cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_mistral_math(input_str, **kwargs):\n",
    "    model_inputs = tokenizer([input_str], return_tensors=\"pt\")\n",
    "    model_inputs = {k: v.to(backbone_model.device) for k, v in model_inputs.items()}\n",
    "    model_outputs = backbone_model.generate(\n",
    "        **model_inputs,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=1000,\n",
    "    )\n",
    "    return tokenizer.batch_decode(model_outputs, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_vi2en(vi_text: str) -> str:\n",
    "    for i, j in dict_map.items():\n",
    "        vi_text = vi_text.replace(i, j)\n",
    "    input_ids = tokenizer_vi2en(vi_text, return_tensors=\"pt\").input_ids\n",
    "    output_ids = model_vi2en.generate(\n",
    "        input_ids,\n",
    "        decoder_start_token_id=tokenizer_vi2en.lang_code_to_id[\"en_XX\"],\n",
    "        num_return_sequences=1,\n",
    "        # # With sampling\n",
    "        # do_sample=True,\n",
    "        # top_k=100,\n",
    "        # top_p=0.8,\n",
    "        # With beam search\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    en_text = tokenizer_vi2en.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    en_text = \" \".join(en_text)\n",
    "    return en_text\n",
    "\n",
    "translate_vi2en(\"Một cửa hàng đã bán 30% số hàng hiện có và thu được 15 000 000 đồng.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('BAAI/bge-large-en-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect(\"Một cửa hàng đã bán 30% số hàng hiện có và thu được 15 000 000 đồng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "INPUT_STR_PROMPT = \"\"\"Question: {question}\n",
    "Options:\n",
    "{choices[0]}\n",
    "{choices[1]}\n",
    "{choices[2]}\n",
    "{choices[3]}\"\"\"\n",
    "\n",
    "def get_instruction_str(sample):\n",
    "    num_options = len(sample[\"choices\"])\n",
    "    option_chars = [chr(ord(\"A\") + i) for i in range(num_options)]\n",
    "    return f\"Answer the following question for me by choosing option {', '.join(option_chars[:-1])}, or {option_chars[-1]}.\"\n",
    "\n",
    "def get_input_str(sample):\n",
    "    num_options = len(sample[\"choices\"])\n",
    "    INPUT_STR_PROMPT = \"\"\"Question: {question}\n",
    "Options:\n",
    "\"\"\"\n",
    "    for i in range(num_options):\n",
    "        INPUT_STR_PROMPT += \"{\" + f\"choices[{i}]\" + \"}\\n\"\n",
    "    INPUT_STR_PROMPT = INPUT_STR_PROMPT[:-1]\n",
    "    return INPUT_STR_PROMPT.format(**sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_str(sample):\n",
    "    instruction_str = get_instruction_str(sample)\n",
    "    input_str = get_input_str(sample)\n",
    "    return PROMPT_DICT[\"prompt_input\"].format(instruction=instruction_str, input=input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lang(text):\n",
    "    try: \n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        lang = None\n",
    "    return lang\n",
    "\n",
    "def process_option(option, lower=False):\n",
    "    option = option[2:].strip()\n",
    "    return option.lower() if lower else option\n",
    "\n",
    "def preprocess_sample(sample, debug=False):\n",
    "    question = sample[\"question\"]\n",
    "    choices = sample[\"choices\"]\n",
    "    choices = [process_option(choice, lower=False) for choice in choices]\n",
    "    \n",
    "    if detect_lang(question) != \"en\":\n",
    "        if debug: print(f\"Translating question: {question}\")\n",
    "        question = translate_vi2en(question)\n",
    "    \n",
    "    for i, choice in enumerate(choices):\n",
    "        if debug: print(f\"Translating choice {i}: {choice}\")\n",
    "        if detect_lang(choice) != \"en\":\n",
    "            choices[i] = translate_vi2en(choice)\n",
    "    \n",
    "    choices = [chr(ord(\"A\") + i) + \". \" + choice for i, choice in enumerate(choices)]\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"choices\": choices\n",
    "    }\n",
    "\n",
    "def post_process_output(output, sample):\n",
    "    options = sample[\"choices\"]\n",
    "    options = [process_option(option, lower=True) for option in options]\n",
    "    \n",
    "    option_id = {o:i for i, o in enumerate(options)}\n",
    "    \n",
    "    idx = -1\n",
    "    for option in sorted(options, key=len, reverse=True):\n",
    "        if option in output:\n",
    "            idx = option_id[option]\n",
    "\n",
    "    if idx == -1:\n",
    "        pred = output.split(\"\\n\")[-3:]\n",
    "        pred = \" \".join(pred)\n",
    "        choices = sample[\"choices\"]\n",
    "        embeddings_1 = embedding_model.encode(pred, normalize_embeddings=True)\n",
    "        embeddings_2 = embedding_model.encode(choices, normalize_embeddings=True)\n",
    "        similarity = embeddings_1 @ embeddings_2.T\n",
    "        idx = np.argmax(similarity)\n",
    "    return idx\n",
    "    \n",
    "def answer(sample, math_llm, num_trials=2, debug=False):\n",
    "    processed_sample = preprocess_sample(sample, debug)\n",
    "    \n",
    "    model_str = get_model_str(processed_sample)\n",
    "    if debug: print(model_str)\n",
    "    \n",
    "    for i in range(num_trials):\n",
    "        math_output = math_llm(\n",
    "            model_str,\n",
    "            top_k=1,\n",
    "            max_tokens=1024,\n",
    "        )\n",
    "        try:\n",
    "            output = math_output[\"choices\"][0][\"text\"].strip()\n",
    "        except:\n",
    "            output = math_output\n",
    "        if output != \"\" and i > 0:\n",
    "            if debug: print(output)\n",
    "            break\n",
    "\n",
    "    idx = post_process_output(output, processed_sample)\n",
    "    result = {\n",
    "        \"id\" : sample[\"id\"],\n",
    "        \"answer\": sample[\"choices\"][idx]\n",
    "    }\n",
    "\n",
    "    if debug: print(result)\n",
    "    \n",
    "    return result, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = json.load(open(\"./datasets/public_test/math_test.json\", \"r\", encoding=\"utf-8\"))['data']\n",
    "test_samples[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer(test_samples[3], math_model, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "answer_data = []\n",
    "logging_data = []\n",
    "for sample in tqdm(test_samples):\n",
    "    debug = True if random.random() < 0.1 else False\n",
    "    ans, model_output = answer(sample, math_model, debug=debug)\n",
    "    if debug:\n",
    "        print(\"*\" * 50 + \"\\n\\n\")\n",
    "    answer_data.append(ans)\n",
    "\n",
    "    sample[\"model_output\"] = model_output\n",
    "    logging_data.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "submission_df = pd.DataFrame(answer_data)\n",
    "submission_df.to_csv(\"./submissions/baseline1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./logs/baseline1.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(logging_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
